\documentclass[11pt, a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage[all]{hypcap}
\usepackage[space]{grffile}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{nccmath, mathtools}
\usepackage{amsthm,amssymb}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage[title]{appendix}

\newlist{questions}{enumerate}{1}
\setlist[questions, 1]{label = \arabic*}
\newlist{bonus}{enumerate}{1}
\setlist[bonus, 1]{label = Bonus \arabic*}

\makeatletter
\def\namedlabel#1#2{\begingroup
	\def\@currentlabel{#2}%
	\phantomsection\label{#1}\endgroup
}
\makeatother

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ 
	backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
	basicstyle=\footnotesize,        % the size of the fonts that are used for the code
	breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
	breaklines=true,                 % sets automatic line breaking
	captionpos=b,                    % sets the caption-position to bottom
	commentstyle=\color{mygreen},    % comment style
	deletekeywords={...},            % if you want to delete keywords from the given language
	escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
	extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	frame=single,	                   % adds a frame around the code
	keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	keywordstyle=\color{blue},       % keyword style
	language=Octave,                 % the language of the code
	morekeywords={*,...},            % if you want to add more keywords to the set
	numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
	numbersep=5pt,                   % how far the line-numbers are from the code
	numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
	rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
	showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
	showstringspaces=false,          % underline spaces within strings only
	showtabs=false,                  % show tabs within strings adding particular underscores
	stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
	stringstyle=\color{mymauve},     % string literal style
	tabsize=2,	                   % sets default tabsize to 2 spaces
	title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

% Adjust margins
\addtolength{\oddsidemargin}{-0.75in}
\addtolength{\evensidemargin}{-0.75in}
\addtolength{\textwidth}{1.5in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1.5in}
\setlength\parindent{0pt}
\setlength{\parskip}{5pt}

\title{CS 536 : Final Project - Data Completion and Interpolation}
\author{Haoyang Zhang, Han Wu, Shengjie Li}
\date{\today}

\begin{document}
\maketitle

\section{Introduction, group members and division of workload}
\label{sec:Introduction}

In this group project, we implemented an autocoder for interpolating missing features from features we have and achieved . \\
\begin{tabular}{| p{2.7cm} | p{\textwidth -3.5cm} |}
	\hline
	\makecell[c]{Name \\ NetID} & Workload \\
	\hline
	\makecell[c]{Han Wu \\ hw436} & {Fine-tuned the parameters of our model. Did some experiments for the evaluation of our model. Wrote part of the report.} \\
	\hline
	\makecell[c]{Haoyang Zhang \\ hz333} & {Analyzed and wrote scripts to clean the data. Wrote scripts to restore human-friendly data from the output of our model. Wrote part of the report. } \\
	\hline
	\makecell[c]{Shengjie Li \\ sl1560} & {Implemented the basics of neural networks including back-propagation and several loss functions and activation functions. Wrote part of the report.} \\
	\hline
\end{tabular}

\section{Data preprocessing and thoughts towards this project}
\label{sec:Prerequisites}
\begin{enumerate}
	\item {How to represent or process the data. Data features may contain a number of diverse data types (real values, integer values, categorical or binary values, ordered categorical values, open/natural language responses). How can you represent these for processing and prediction?}
	\begin{itemize}
		\item {\textbf{Data encoding}}
		\par{Given the \colorbox{lightgray}{ML3AllSites} dataset, we can classify each column into 6 types: (positive) integers, unordered multiple choices, Boolean-like values, long texts, other valid responses and NA's.
			
			(Note: the 1177th row is wrongly encoded in the original dataset, but we fixed it in the \colorbox{lightgray}{ML3AllSitesC}. However, it again includes some more bugs in date related columns like column 124 Date.x because of Microsoft Excel. Anyway, we eventually fixed them when reading the dataset when calling \colorbox{lightgray}{dataFormat.py})
			
			A detailed codebook is available in Appendix \ref{appendix:codebook}, and further reference is in \colorbox{lightgray}{/code/dataFormat.py}, which is organized in the order of original dataset.}
		\begin{itemize}
			\item {(Postive) Integers}
			\par{Many columns belong to this type, for examples, best grade 1 (column 21), mcdv1 (column 71), temperature in lab (column 126) and intrinsic (column 247). If the choices are exactly ordered, say ``1'' is ``unhappy'', and ``10'' is ``happy'', we consider this column as this type. Tough ``2'' may not be twice happier than ``1'', nevertheless, 2 is indeed happier. Note that some columns may include float numbers, say intrinsic, but we can multiply a factor to scale all responses to integers. Also, some responses could be negative values, say mcdv1, but we still can add a number to shift all of them to non-negative values.}
			\par{Each value in these columns is the real value instead of probability.}
			\item {Unordered Multiple Choices}
			\par{Many columns belong to this type, for examples, ethnicity (column 42), gender (column 44), major (column 70) and V position (column 115). Note that these columns may include natural language response, say major, but we have classified all responses into several choices.}
			\par{Each choice of these columns are exclusive and unordered. Therefore, we cannot just simply encode them into integers. Or we will have to face the explanatory problem: If we encode ``computer science'' into ``1'' and ``mathematics'' into ``2'', do we mean a ``mathematics'' is equal to 2 ``computer science''? Therefore, we choose one-hot encoding to use the same number as choices of Boolean values to represent the participant’s choice. In this case, we can consider each value as the probability that this participant will choose this response. }
			\item {Boolean-like Values}
			\par{A few columns belong to this type. Some are natural language responses but there is a true answer, say anagrams (column 5 and 6) and attention correct (column 10), and the test is highly concerning about whether the participant correct or not instead of what they answered. Others are multiple choices with exactly 2 possible answers like mcmost (column 76 to 80), and for simplicity we prefer to use 1 Boolean value to represent his/her choice.}
			\par{In general, we can consider this type as a special multiple choice type. Namely, each value in this column is a probability.}
			\item {Long Texts}
			\par{There are exactly 3 columns belongs to this type: highpower (column 45), lowpower (column 67) and Notes (column 134). Because of time limition, we skiped to process these 3 columns.}
			\item {Other Valid Responses}
			\par{Some natural language responses that describe a real number belong to this type, for example, K ratio (column 66), worst grade 2 (column 118) and SR TF Correct (column 133). Some obviously unrelated or redundant data is also this type. For an example, Date Computer (column 220) is a duplicate to Month Computer, Day Computer and Year Computer (column 222 to 224).}
			\item {NA's}
			\par{In order to distinguish normal data and NA's, we use a valid mask. For each encoded feature, we use a Boolean value to indicate whether it is normal or NA. Namely, each row in original dataset is coded into 2 rows, where one is a valid mask and the other is the real data.}
			\par{For simplicity, we set all NA’s to 0 just like dropout. When computing error, we use the mask to set these features’ loss to 0.}
		\end{itemize}
		\item {\textbf{Data preprocessing}}
		\namedlabel{preprocessing}{Data preprocessing}
		\begin{itemize}
			\item {Scaling to $[0, 1]$}
			\par{Notice that the coded data can be further devided into 2 types: real values and probabilities. Notice that real values could be from negative infinity to positive infinity. (There may be some more restrictions like temperature in lab cannot be lower than -460. But in general its range is way larger than $[0, 1]$.) But for our prediction simplicity, we will linearly scale the largest value seen to $1$ and the smallest to $0$.}
			\par{Here are some more things we can do, but because of time limit, we skipped them. The straightforward problem for this naïve scaling is we might be trapped by outliers. For example, (this example is already fixed.) some participant claimed his/her/its age is about 150. If we directly apply the scaling, most 20-ish responses will be scaled into about $0.007$, and the only response that is greater than $0.3$ is that 150, which will make this feature hard to predict precisely. Therefore, we should throw out these outliers.}
			\par{But a further thought is that this situation can also happen when the response distributed unevenly. For example, many people answer either about 1 to 2 or 8 to 9. In this case, we use a lot of space to encode unlikely values, which leads to the same result. In this case, a nonlinear scale method will be helpful. A rudimentary thought is we sort all values in the dataset and linearly scale first 10\% values to the range $[0, 0.1)$, second 10\% to $[0.1, 0.2)$, and so on.}
			\par{Another problem is that we cannot predict any larger or smaller values than values in the dataset. A plausible justification could be that it is generally unlikely to see an extreme small or large value. But if we adopt the nonlinear scale method, we can map negative infinity (or the smallest valid value) to the smallest value in dataset into $[0, 0.1)$, and all values in dataset to $[0.1, 0.9)$, and so on.}
			\item {Grouping Features}
			\par{Notice that this dataset is all about 10 psychological tests. Therefore, we can assume the features in the same tests are more related than features between different tests. Namely the dimensionality in each tests is relatively small. Therefore, we can group features in terms of tests. Some global information about this participant, like demographic features and personality features, is grouped into another global set instead of test sets, which is called group 0. Some definitely unrelated data like participant ID (column 1) is grouped into another set, called group 11.}
			\par{In this way, we can try to use group 0 and each test group to predict blank features in this test just by picking 2 group indices instead of a huge number of feature indices.}
			\item {Selecting Features}
			\par{Notice that all group 11 features can be discard based on our prior knowledge. (Actually we should use graphical model to prove it.) A further thing we should do is run Chow Liu Algorithm on group 0 and each test group to find weak-dependent intra-test features (Namely, all its edges are weak.) and eliminate them, and then run it on the whole feature space to try to further eliminate features.}
			\item {Discarding Almost NA Data Points}
			\par{Notice that there are several almost blank rows in the original datasets. These data points cannot tell us many things. We can use NA masks to identify them. More specifically, we remove these rows whose mask has more than $100$ \colorbox{lightgray}{False}.}
		\end{itemize}
	\end{itemize}
	
	\item{How to model the problem of interpolation. What are the inputs, what are the outputs? An important if subtle question to consider here - what does it mean to predictor or interpolate a missing feature?}
	\namedlabel{interpolation}{Formalization of interpolation}
	\par{Given the preprocessed data, each data point is a vector with some blanks, and the whole dataset is a matrix with blanks. Our goal is to fill the blanks. Notice that we can do this because the dimensionality of this matrix is limited. Namely, many features are related to each other. For example, those who claimed they are high self-esteemed (column 252) are generally less stressed (column 253) and their mood (column 248) is better. Therefore, we can consider the dataset as a limited-rank matrix with some blanks whose size is $2434 \times 261$.}
	\par{A straight forward idea to this problem is consider the blanks as noise, and our goal becomes to detect and eliminate this noise. Notice that the rank of this matrix is limited, therefore we can transform it into a much smaller matrix and restore it. Assuming the noise is relatively smaller than the information that this matrix gives, when we are transform or compressing this matrix, the noise will be eliminated, and then we can decompress it to restore the blanked values.}
	
	\item{Model selection. What kind of model or models do you want to consider?}
	\par{Before we started doing this project, we discussed several models.}
	\begin{itemize}
		\item {Autoencoders}
		\par{It can compress and decompress the given inputs into smaller representations, and these representations contain key information of the inputs. Thus, we can make use of these representations to classify data and generate data, which makes autoencoders handy in this situation.}
		\item {A transfer learning approach:}
		\par{First, we can train an autoencoder to compress information from the inputs, minimize the reconstruction loss. After we finish the training of the autoencoder, we discard the decoder, and use the encoder to get representations of data points. Then, we train a classifier on top of the representations.}
		\item {Autoencoders with RNN:}
		\par{Since the data contains natural language data, we could use RNN to deal with this kind of data.}
	\end{itemize}
\end{enumerate}


\section{Requirements}
\label{sec:Requirements}
\begin{enumerate}
	\begin{figure}[H]
		\centering
		\includegraphics[width=\linewidth]{network.png}
		\caption{Our autoencoder}
		\label{pic:model}
	\end{figure}
	\item {\textbf{Describe your Model:} What approach did you take? What design choices did you make, and why? How
		did you represent the data? How can you evaluate your model for goodness of fit? Did you make an effort to
		identify and exclude irrelevant variables? How did you handle missing data?}
	\par{Given the \ref{interpolation}, we eventually chose to use an autoencoder as our model because it can compress and decompress the given inputs and it is not complicated to build.} 
	\begin{itemize}
		\item {Structure}
		\par{Figure \ref{pic:model} shows the structure of our model.}
		\begin{enumerate}
			\item {The input}
			\par{There are 223 nodes in the input. We scaled the input to be in $ [0, 1] $. Each node could be either in a classification group or in a real value group. The activation function here is ReLU in order to make it less computationally expensive. The dropout rate of this layer is 10\%.}
			\item {Hidden layer 1}
			\par{There are 180 nodes in this layer. The activation function here is ReLU.}
			\item {Hidden layer 2}
			\par{There are 100 nodes in this layer. The activation function here is ReLU.}
			\item {Hidden layer 3}
			\par{There are 180 nodes in this layer. The activation function here is ReLU.}
			\item {The output}
			\par{There are 223 node in the output.  The activation function here is Sigmoid in order to make values lie in $ [0, 1] $.}
		\end{enumerate}
		
		\item {Loss/Error}
		\par{We are measuring the reconstruction errors. For the reason that there are many data types, we are treating the data over different loss functions.}
		\par{For real values and integer values, we are using mean squared error (m denotes the number of data points) \[ L_{MSE}(\theta) = \frac{1}{m}\sum_{i=1}^{m}(\underline{y}^i - \underline{p}^i)^2 , \]   and root-mean-squared error \[ L_{RMSE}(\theta) = \sqrt{L_{MSE}(\theta)}. \]}
		\par{For categorical values, we first converted them to one-hot encoding, then we are treating the problem as a multi-label classification problem (for these categorical values only). Thus, for categorical values, we are using binary cross-entropy loss \[ L_{BCE}(\theta) = \frac{1}{m}\sum_{i=1}^{m}[\underline{y}^i \log (\underline{p}^i) + (1 - \underline{y}^i) \log (1 - \underline{p}^i)]). \]}
		\par{Because the data set is a combination of different types of data, we are using a combination of different loss functions.}
		
		\item {Data}
		\par{This is included in \ref{preprocessing}. We are only training over part of the features. For real values data, we scale it into $ [0, 1] $. For categorical data, we used one-hot encoding to represent which also make the data lie in $ [0, 1] $. Basically all data are in $ [0, 1] $ and we treated all of them as float numbers. }
		\par{We manually removed some irrelevant features such as participant ID. We don't think we are able to train over these features.}
		\par{When dealing with missing features, we have a mask to let us know which feature is missed and then we don't need to calculate loss on this feature.}
	
		\item {Other facts}
		\par{Since we already grouped features as Figure \ref{pic:model} shows, we can first encode each groups and then encode the whole data points. In this way, we can dramatically eliminate the number of weights because our first layer is not dense.}
		\par{Notice that we encode the multiple choice columns into several features. Therefore, we can append a softmax layer to each multiple choice column. One thing to notice that for non-choice features, we have to directly output its value instead of pass it to softmax layer, or it will always return 1. (When we are writing this report, we notice that we can expend each non-choice features into 2 features $x$ and $\bar x = 1-x$. In this case, we can directly pass all features of the same column to a softmax, instead of using masks to identify non-choice features.)}
		\par{Here is another interesting but not necessarily useful fact. Notice that we assumed the rank of this matrix is limited. Namely the transformation can actually be linear. Namely we literally can multiply 2 small matrices to get this matrix. Therefore, we can further define this problem as find 2 matrices $A$ and $B$ to minimize $||M - AB||$, which is literally an optimization problem. But the drawback is that we ignore the meaning of each feature, say sum of several features should always be $1$. But it might be not a bad start, and we can further use Ada boost to combine it with our AutoEncoder.}
	\end{itemize}
	
	\item {\textbf{Describe your Training Algorithm:}}
	\par{We implemented a gradient-based optimizer--stochastic gradient descent with back-propagation for the training part. Comparing with other types of gradient descent, stochastic gradient descent uses one data point at a time, which is computationally friendly. We are only training over 223 features. Our training process was actually quite fast, so we didn't do any compromises.}
	
	\item {\textbf{Describe your Model Validation:}}
	\par{We divided the whole data set into 3 data sets: train set, dev set and test set. During the training process, we measure loss on train set and dev set. Dev set is an indicator of fitness and has been used in fine-tuning of the hyperparameters. If dev loss starts to climb up, we could know it is overfitting. After we finish the training of the model, we evaluate loss on test set.}
	\par{For the prevention of overfitting, we are using a simple architecture, and we added drop-out rates on layers to enable the model to `disable' some neurons for some probabilities. Because we are dealing with missing data and trying to interpolate missing data, drop-out makes the model be used to incomplete input data. Since the dataset isn't massive, drop-out `creates' many incomplete data point, which boosts the size of the dataset.}
	\par{After the training, our model achieved a low loss of 20-ish.}
	
	\item {\textbf{Evaluate your Model:}}
	
	\includepdf[pages=-,pagecommand={},width=\textwidth]{hanwu.pdf}
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.4\linewidth]{ana.png}
		\caption{Loss of each feature to the original data}
		\label{easyToPredict}
	\end{figure}
	\item {\textbf{Analyze the Data:}}
	\par{Are there some features it is really good at predicting and some it is really poor at predicting? Why do you think that is?}
	\par{In order to answer this question, we traced the loss of each feature to the original data, and the result is shown in Figure \ref{easyToPredict}: }
	\par{Notice that the best real-value feature we predict is L ratio (column 69), and it is really diverse. A plausible explanation is that this effect is ``strong'' enough, and it is true according to the original study (ML3). Actually, the best 3 features (L, K, R ratio) all belong to this effect.}
	\par{The worst real-value feature is sarcasm (column 103). Notice that this effect includes a number of natural language response which we preprocessed roughly, and this effect is also not that significant. Therefore, it is reasonable we cannot predict this feature well.}
	\par{Column 7 and 8 are anagrams 3 and 4, which we encode any word into ``correct answer''. Therefore, it should be the best 2 features because it can literally always predict ``correct answer''. Except these 2 feature, the best probability feature is clipboard weight (column 128), it is interesting because there is no directly related feature except the participant answers. (Note that the clipboard material is independent to its weight.) Namely, we do find that the weight of clipboard will influence participant’s response.}
	\par{The worst probability feature is persistence. (It is a bug actually because I preprocessed it into a probability feature but it actually is a real value. It is my fault. But a good story is it shows we should not use cross entropy to measure the loss of a real-value feature.) The worst feature except for that bugged one is anagrams 1 (column 5). A plausible explanation is that this feature is sort of coincident because we may either happen to have the ``Eureka'' time instantly or be puzzled for a long time and quit this question.}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.4\linewidth]{2.png}
		\caption{feature importance}
		\label{2}
	\end{figure}
	
	\par{What features were particularly valuable in predicting/interpolating? What features weren’t particularly useful at all?}
	
	\par{In order to answer this question, we dropout each feature and calculating the loss after we dropout it. In case of the situation that this feature itself is hard to predict, when we calculating the loss, we will also ignore this feature. The result is in Figure \ref{2}. )
		
		Notice that most of the most important real-value features are aggregated features like NFC, Mood and Agreeableness. It is reasonable because they tell us a whole aspect of this participant, and the response of this participant can be consider as a function of all aspects. (Refer to our transfer learning thought.) Similarly, we can find that most of the least important real-value features are the actual response of each aggregated feature, like intrinsic\_01, selfesteem\_01 and soon. Since they have been aggregated into another features, they do tell us less.
		
		Notice that order of tasks (column 127) does important. It corresponds to the original study design. Some other important features are N position and V position. It is also reasonable because it will determine whether the next response will be greater than 10 or less than 10.
		
		Again, because of my fault, persistence (column 171) become the least important feature. Except for this, the back count features are generally less important. A justification is most participant will correctly answer this question.}
	
	\item {\textbf{Generate Data:}}
	\par{There are two approaches:}
	\begin{enumerate}
		\item {}
		\par{We picked a complete datapoint and removed part of it, used this to predict the removed features. Then we compared the predicted features and the original features. If they are similar, then we say it's a successful generation. }
		\par{Our model was quite good at this job.}
		\item {}
		\par{To generate a new data point, first we picked a complete data point, then we removed part of it and use the removed data point to predict the removed part. Then we combined the removed data point and the prediction and repeat this process. At last all data from the original data point was removed. }
		\par{To determine whether it's good, we compare the new data point with the original data point. If they are similar, then we say it's a successful generation.}
		\par{Our model was doing alright at this job.}
	\end{enumerate}
\end{enumerate}


\section{Bonus}
\label{sec:Bonus}

\begin{appendices}
	\section{Codebook}
	\label{appendix:codebook}
	\includepdf[pages=-,pagecommand={},width=\textwidth]{codebook.pdf}
\end{appendices}


\end{document}